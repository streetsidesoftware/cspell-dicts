# Snapshot report for `test/verify.test.mjs`

The actual snapshot is saved in `verify.test.mjs.snap`.

Generated by [AVA](https://avajs.dev).

## verify samples/issue-1097.tex against snapshot

> Snapshot 1

    [
      'I _     0-    9 | % [Issue ',
      'E _     9-   14 | #1097',
      'I _    14-   16 | ](',
      'E _    16-   79 | https://github.com/streetsidesoftware/cspell-dicts/issues/1097)',
      `I _    79-   81 | ␊
      ␊
      `,
      'E _    81-   96 | $p^1_1\\in[0,2)$',
      `I _    96-   98 | ␊
      ␊
      `,
      'E _    98-  110 | \\skip[skipp]',
      `I _   110-  118 | ␊
      ␊
      good.␊
      `,
    ]

## verify samples/papers/write-math-paper/write-math-ba-paper.tex against snapshot

> Snapshot 1

    [
      'E _     0-   46 | \\documentclass[9pt,technote,a4paper]{IEEEtran}',
      `I _    46-   47 | ␊
      `,
      'E _    47-   76 | \\usepackage{amssymb, amsmath}',
      `I _    76-   96 |  % needed for math␊
      ␊
      `,
      'E _    96-  119 | \\usepackage[a-1b]{pdfx}',
      `I _   119-  120 | ␊
      `,
      'E _   120-  145 | \\usepackage{filecontents}',
      `I _   145-  146 | ␊
      `,
      'E _   146-  185 | \\begin{filecontents*}{\\jobname.xmpdata}',
      `I _   185-  190 | ␊
          `,
      'E _   190-  291 | \\Keywords{recognition\\sep machine learning\\sep neural networks\\sep symbols\\sep multilayer perceptron}',
      `I _   291-  296 | ␊
          `,
      'E _   296-  302 | \\Title',
      `I _   302-  364 | {On-line Recognition of Handwritten Mathematical Symbols}␊
          `,
      'E _   364-  418 | \\Author{Martin Thoma, Kevin Kilgour, Sebastian St{\\"u}',
      `I _   418-  448 | ker and Alexander Waibel}␊
          `,
      'E _   448-  495 | \\Org{Institute for Anthropomatics and Robotics}',
      `I _   495-  500 | ␊
          `,
      'E _   500-  506 | \\Doi{}',
      `I _   506-  507 | ␊
      `,
      'E _   507-  526 | \\end{filecontents*}',
      `I _   526-  528 | ␊
      ␊
      `,
      'E _   528-  550 | \\RequirePackage{ifpdf}',
      `I _   550-  551 | ␊
      `,
      'E _   551-  557 | \\ifpdf',
      'I _   557-  558 |  ',
      'E _   558-  604 | \\PassOptionsToPackage{pdfpagelabels}{hyperref}',
      'I _   604-  605 |  ',
      'E _   605-  608 | \\fi',
      `I _   608-  609 | ␊
      `,
      'E _   609-  634 | \\RequirePackage{hyperref}',
      `I _   634-  635 | ␊
      `,
      'E _   635-  655 | \\usepackage{parskip}',
      `I _   655-  656 | ␊
      `,
      'E _   656-  691 | \\usepackage[pdftex,final]{graphicx}',
      `I _   691-  692 | ␊
      `,
      'E _   692-  713 | \\usepackage{csquotes}',
      `I _   713-  714 | ␊
      `,
      'E _   714-  733 | \\usepackage{braket}',
      `I _   733-  734 | ␊
      `,
      'E _   734-  755 | \\usepackage{booktabs}',
      `I _   755-  756 | ␊
      `,
      'E _   756-  777 | \\usepackage{multirow}',
      `I _   777-  778 | ␊
      `,
      'E _   778-  799 | \\usepackage{pgfplots}',
      `I _   799-  800 | ␊
      `,
      'E _   800-  820 | \\usepackage{wasysym}',
      `I _   820-  821 | ␊
      `,
      'E _   821-  841 | \\usepackage{caption}',
      `I _   841-  844 | ␊
      % `,
      'E _   844-  857 | \\captionsetup',
      `I _   857-  888 | {belowskip=12pt,aboveskip=4pt}␊
      `,
      'E _   888-  901 | \\makeatletter',
      `I _   901-  902 | ␊
      `,
      'E _   902-  954 | \\newcommand\\mynobreakpar{\\par\\nobreak\\@afterheading}',
      `I _   954-  955 | ␊
      `,
      'E _   955-  967 | \\makeatother',
      `I _   967-  968 | ␊
      `,
      'E _   968-  995 | \\usepackage[noadjust]{cite}',
      `I _   995-  996 | ␊
      `,
      'E _   996- 1038 | \\usepackage[nameinlink,noabbrev]{cleveref}',
      `I _  1038- 1084 |  % has to be after hyperref, ntheorem, amsthm␊
      `,
      'E _  1084- 1138 | \\usepackage[binary-units,group-separator={,}]{siunitx}',
      `I _  1138- 1139 | ␊
      `,
      'E _  1139- 1184 | \\sisetup{per-mode=fraction,binary-units=true}',
      `I _  1184- 1185 | ␊
      `,
      'E _  1185- 1209 | \\DeclareSIUnit\\pixel{px}',
      `I _  1209- 1210 | ␊
      `,
      'E _  1210- 1233 | \\usepackage{glossaries}',
      `I _  1233- 1234 | ␊
      `,
      'E _  1234- 1265 | \\loadglsentries[main]{glossary}',
      `I _  1265- 1266 | ␊
      `,
      'E _  1266- 1281 | \\makeglossaries',
      `I _  1281- 1283 | ␊
      ␊
      `,
      'E _  1283- 1289 | \\title',
      `I _  1289- 1347 | {On-line Recognition of Handwritten Mathematical Symbols}␊
      `,
      'E _  1347- 1401 | \\author{Martin Thoma, Kevin Kilgour, Sebastian St{\\"u}',
      `I _  1401- 1428 | ker and Alexander Waibel}␊
      ␊
      `,
      `E _  1428- 1510 | \\hypersetup{␊
        pdfauthor   = {Martin Thoma\\sep Kevin Kilgour\\sep Sebastian St{\\"u}`,
      'I _  1510- 1513 | ker',
      'E _  1513- 1517 | \\sep',
      `I _  1517- 1565 |  Alexander Waibel},␊
        pdfkeywords = {recognition`,
      'E _  1565- 1569 | \\sep',
      'I _  1569- 1586 |  machine learning',
      'E _  1586- 1590 | \\sep',
      'I _  1590- 1606 |  neural networks',
      'E _  1606- 1610 | \\sep',
      'I _  1610- 1618 |  symbols',
      'E _  1618- 1622 | \\sep',
      `I _  1622- 1755 |  multilayer perceptron},␊
        pdfsubject  = {Recognition},␊
        pdftitle    = {On-line Recognition of Handwritten Mathematical Symbols},␊
      }␊
      `,
      'E _  1755- 1774 | \\include{variables}',
      `I _  1774- 1775 | ␊
      `,
      'E _  1775- 1806 | \\crefname{table}{Table}{Tables}',
      `I _  1806- 1807 | ␊
      `,
      'E _  1807- 1841 | \\crefname{figure}{Figure}{Figures}',
      `I _  1841- 2053 | ␊
      ␊
      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%␊
      % Begin document                                                    %␊
      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%␊
      `,
      'E _  2053- 2069 | \\begin{document}',
      `I _  2069- 2070 | ␊
      `,
      'E _  2070- 2080 | \\maketitle',
      `I _  2080- 2081 | ␊
      `,
      'E _  2081- 2097 | \\begin{abstract}',
      `I _  2097- 2612 | ␊
      The automatic recognition of single handwritten symbols has three main␊
      applications: Supporting users who know how a symbol looks like, but not what␊
      its name is, providing the necessary commands for professional publishing, or␊
      as a building block for formula recognition.␊
      ␊
      This paper presents a system which uses the pen trajectory to classify␊
      handwritten symbols. Five preprocessing steps, one data multiplication␊
      algorithm, five features and five variants for multilayer Perceptron training␊
      were evaluated using `,
      'E _  2612- 2626 | $\\num{166898}$',
      `I _  2626- 2814 |  recordings. Those recordings were made␊
      publicly available. The evaluation results of these 21~experiments were used to␊
      create an optimized recognizer which has a top-1 error of less than␊
      `,
      'E _  2814- 2835 | $\\SI{17.5}{\\percent}$',
      'I _  2835- 2857 |  and a top-3 error of ',
      'E _  2857- 2877 | $\\SI{4.0}{\\percent}$',
      `I _  2877- 2913 | . This is a␊
      relative improvement of `,
      'E _  2913- 2934 | $\\SI{18.5}{\\percent}$',
      `I _  2934- 2959 |  for the top-1 error and␊
      `,
      'E _  2959- 2980 | $\\SI{29.7}{\\percent}$',
      `I _  2980- 3067 |  for the top-3 error compared to the baseline system. This␊
      improvement was achieved by `,
      'E _  3067- 3080 | \\acrlong{SLP}',
      `I _  3080- 3146 |  and adding new features. The␊
      improved classifier can be used via `,
      'E _  3146- 3192 | \\href{http://write-math.com/}{write-math.com}.',
      `I _  3192- 3193 | ␊
      `,
      'E _  3193- 3207 | \\end{abstract}',
      `I _  3207- 3209 | ␊
      ␊
      `,
      'E _  3209- 3233 | \\input{ch1-introduction}',
      `I _  3233- 3234 | ␊
      `,
      'E _  3234- 3267 | \\input{ch2-general-system-design}',
      `I _  3267- 3268 | ␊
      `,
      'E _  3268- 3303 | \\input{ch3-data-and-implementation}',
      `I _  3303- 3304 | ␊
      `,
      'E _  3304- 3326 | \\input{ch4-algorithms}',
      `I _  3326- 3327 | ␊
      `,
      'E _  3327- 3368 | \\input{ch5-optimization-of-system-design}',
      `I _  3368- 3369 | ␊
      `,
      'E _  3369- 3388 | \\input{ch6-summary}',
      `I _  3388- 3389 | ␊
      `,
      'E _  3389- 3411 | \\input{ch7-mfrdb-eval}',
      `I _  3411- 3414 | ␊
      ␊
      ␊
      `,
      'E _  3414- 3444 | \\bibliographystyle{IEEEtranSA}',
      `I _  3444- 3445 | ␊
      `,
      'E _  3445- 3479 | \\bibliography{write-math-ba-paper}',
      `I _  3479- 3480 | ␊
      `,
      'E _  3480- 3494 | \\end{document}',
      `I _  3494- 3495 | ␊
      `,
    ]

## verify samples/papers/write-math-paper/variables.tex against snapshot

> Snapshot 1

    [
      'E _     0-   46 | \\newcommand{\\totalCollectedRecordings}{166898}',
      `I _    46-   60 |   % ACTUALITY␊
      `,
      'E _    60-  109 | \\newcommand{\\detexifyCollectedRecordings}{153423}',
      `I _   109-  110 | ␊
      `,
      'E _   110-  147 | \\newcommand{\\trainingsetsize}{134804}',
      `I _   147-  148 | ␊
      `,
      'E _   148-  185 | \\newcommand{\\validtionsetsize}{15161}',
      `I _   185-  186 | ␊
      `,
      'E _   186-  218 | \\newcommand{\\testsetsize}{17012}',
      `I _   218-  219 | ␊
      `,
      'E _   219-  251 | \\newcommand{\\totalClasses}{1111}',
      `I _   251-  252 | ␊
      `,
      'E _   252-  291 | \\newcommand{\\totalClassesAnalyzed}{369}',
      `I _   291-  292 | ␊
      `,
      'E _   292-  333 | \\newcommand{\\totalClassesAboveFifty}{680}',
      `I _   333-  334 | ␊
      `,
      'E _   334-  386 | \\newcommand{\\totalClassesNotAnalyzedBelowFifty}{431}',
      `I _   386-  387 | ␊
      `,
      'E _   387-  442 | \\newcommand{\\detexifyPercentage}{$\\SI{91.93}{\\percent}$',
      `I _   442-  444 | }␊
      `,
      'E _   444-  498 | \\newcommand{\\recordingsWithDots}{$\\SI{2.77}{\\percent}$',
      `I _   498-  522 | }  % excluding i,j, ...␊
      `,
      'E _   522-  586 | \\newcommand{\\recordingsWithDotsSizechange}{$\\SI{0.85}{\\percent}$',
      'I _   586-  609 | }  % excluding i,j, ...',
    ]

## verify samples/papers/write-math-paper/sample-escape-math.tex against snapshot

> Snapshot 1

    [
      `I _     0-   10 | Example:␊
      ␊
      `,
      'E _    10-   13 | $g$',
      'I _    13-   16 |  \\\\',
      'E _    16-   24 | $xyzt=0$',
      'I _    24-   25 |  ',
      'E _    25-   28 | $g$',
      `I _    28-   92 | ␊
      ␊
      \\\\\\$1000 yen -- to be checked.␊
      \\$1000 euros -- to be checked.␊
      `,
    ]

## verify samples/papers/write-math-paper/glossary.tex against snapshot

> Snapshot 1

    [
      `I _     0-   42 | %!TEX root = thesis.tex␊
      %Term definitions␊
      `,
      'E _    42-   90 | \\newacronym{ANN}{ANN}{artificial neural network}',
      `I _    90-   91 | ␊
      `,
      'E _    91-  140 | \\newacronym{CSR}{CSR}{cursive script recognition}',
      `I _   140-  141 | ␊
      `,
      'E _   141-  184 | \\newacronym{DTW}{DTW}{dynamic time warping}',
      `I _   184-  185 | ␊
      `,
      'E _   185-  227 | \\newacronym{GTW}{GTW}{greedy time warping}',
      `I _   227-  228 | ␊
      `,
      'E _   228-  270 | \\newacronym{HMM}{HMM}{hidden Markov model}',
      `I _   270-  271 | ␊
      `,
      'E _   271-  317 | \\newacronym{HWR}{HWR}{handwriting recognition}',
      `I _   317-  318 | ␊
      `,
      'E _   318-  374 | \\newacronym{HWRT}{HWRT}{handwriting recognition toolkit}',
      `I _   374-  375 | ␊
      `,
      'E _   375-  419 | \\newacronym{MLP}{MLP}{multilayer perceptron}',
      `I _   419-  420 | ␊
      `,
      'E _   420-  461 | \\newacronym{MSE}{MSE}{mean squared error}',
      `I _   461-  462 | ␊
      `,
      'E _   462-  502 | \\newacronym{OOV}{OOV}{out of vocabulary}',
      `I _   502-  503 | ␊
      `,
      'E _   503-  553 | \\newacronym{TDNN}{TDNN}{time delay neural network}',
      `I _   553-  554 | ␊
      `,
      'E _   554-  605 | \\newacronym{PCA}{PCA}{principal component analysis}',
      `I _   605-  606 | ␊
      `,
      'E _   606-  657 | \\newacronym{LDA}{LDA}{linear discriminant analysis}',
      `I _   657-  658 | ␊
      `,
      'E _   658-  760 | \\newacronym{CROHME}{CROHME}{Competition on Recognition of Online Handwritten Mathematical Expressions}',
      `I _   760-  761 | ␊
      `,
      'E _   761-  806 | \\newacronym{GMM}{GMM}{Gaussian mixture model}',
      `I _   806-  807 | ␊
      `,
      'E _   807-  852 | \\newacronym{SVM}{SVM}{support vector machine}',
      `I _   852-  853 | ␊
      `,
      'E _   853-  898 | \\newacronym{PyPI}{PyPI}{Python Package Index}',
      `I _   898-  899 | ␊
      `,
      'E _   899-  952 | \\newacronym{CFM}{CFM}{classification figure of merit}',
      `I _   952-  953 | ␊
      `,
      'E _   953-  987 | \\newacronym{CE}{CE}{cross entropy}',
      `I _   987-  988 | ␊
      `,
      'E _   988- 1035 | \\newacronym{GPU}{GPU}{graphics processing unit}',
      `I _  1035- 1036 | ␊
      `,
      'E _  1036- 1096 | \\newacronym{CUDA}{CUDA}{Compute Unified Device Architecture}',
      `I _  1096- 1097 | ␊
      `,
      'E _  1097- 1153 | \\newacronym{SLP}{SLP}{supervised layer-wise pretraining}',
      `I _  1153- 1154 | ␊
      `,
      'E _  1154- 1201 | \\newacronym{AEP}{AEP}{auto-encoder pretraining}',
      `I _  1201- 1222 | ␊
      ␊
      % Term definitions␊
      `,
      'E _  1222- 1265 | \\newglossaryentry{Detexify}{name={Detexify}',
      `I _  1265- 1359 | , description={A system used for␊
      on-line handwritten symbol recognition which is described in `,
      'E _  1359- 1372 | \\cite{Kirsch}',
      `I _  1372- 1376 | }}␊
      ␊
      `,
      'E _  1376- 1413 | \\newglossaryentry{epoch}{name={epoch}',
      'I _  1413- 1478 | , description={During iterative training of a neural network, an ',
      'E _  1478- 1485 | \\textit',
      'I _  1485- 1587 | {epoch} is a single pass through the entire training set, followed by testing of the verification set.',
      'E _  1587- 1603 | \\cite{Concise12}',
      `I _  1603- 1607 | }}␊
      ␊
      `,
      `E _  1607- 1659 | \\newglossaryentry{hypothesis}{␊
          name={hypothesis}`,
      `I _  1659- 1779 | ,␊
          description={The recognition results which a classifier returns is called a hypothesis. In other words, it is the `,
      'E _  1779- 1787 | \\enquote',
      `I _  1787- 1838 | {guess} of a classifier},␊
          plural=hypotheses␊
      }␊
      ␊
      `,
      `E _  1838- 1888 | \\newglossaryentry{reference}{␊
          name={reference}`,
      `I _  1888- 1993 | ,␊
          description={Labeled data is used to evaluate classifiers. Those labels are called references},␊
      }␊
      ␊
      `,
      'E _  1993- 2028 | \\newglossaryentry{YAML}{name={YAML}',
      `I _  2028- 2123 | , description={YAML is a human-readable data format that can be used for configuration files}}␊
      `,
      'E _  2123- 2156 | \\newglossaryentry{MER}{name={MER}',
      'I _  2156- 2256 | , description={An error measure which combines symbols to equivalence classes. It was introduced on ',
      'E _  2256- 2292 | \\cpageref{merged-error-introduction}',
      `I _  2292- 2296 | }}␊
      ␊
      `,
      'E _  2296- 2331 | \\newglossaryentry{JSON}{name={JSON}',
      `I _  2331- 2515 | , description={JSON, short for JavaScript Object Notation, is a language-independent data format that can be used to transmit data between a server and a client in web applications}}␊
      ␊
      `,
      'E _  2515- 2568 | \\newglossaryentry{hyperparamter}{name={hyperparamter}',
      `I _  2568- 2585 | , description={A␊
      `,
      'E _  2585- 2592 | \\textit',
      `I _  2592- 2695 | {hyperparamter} is a parameter of a neural net, that cannot be learned,␊
      but has to be chosen}, symbol={`,
      'E _  2695- 2714 | \\ensuremath{\\theta}',
      `I _  2714- 2718 | }}␊
      ␊
      `,
      'E _  2718- 2771 | \\newglossaryentry{learning rate}{name={learning rate}',
      'I _  2771- 2795 | , description={A factor ',
      'E _  2795- 2817 | $0 \\leq \\eta \\in \\mdr$',
      'I _  2817- 2865 |  that affects how fast new weights are learned. ',
      'E _  2865- 2873 | $\\eta=0$',
      'I _  2873- 2918 |  means that no new data is learned}, symbol={',
      'E _  2918- 2935 | \\ensuremath{\\eta}',
      'I _  2935- 2951 | }} % Andrew Ng: ',
      'E _  2951- 2957 | \\alpha',
      `I _  2957- 2959 | ␊
      ␊
      `,
      'E _  2959- 3024 | \\newglossaryentry{learning rate decay}{name={learning rate decay}',
      'I _  3024- 3063 | , description={The learning rate decay ',
      'E _  3063- 3082 | $0 < \\alpha \\leq 1$',
      'I _  3082- 3155 |  is used to adjust the learning rate. After each epoch the learning rate ',
      'E _  3155- 3161 | $\\eta$',
      'I _  3161- 3176 |  is updated to ',
      'E _  3176- 3207 | $\\eta \\gets \\eta \\times \\alpha$',
      'I _  3207- 3218 | }, symbol={',
      'E _  3218- 3235 | \\ensuremath{\\eta}',
      `I _  3235- 3239 | }}␊
      ␊
      `,
      'E _  3239- 3292 | \\newglossaryentry{preactivation}{name={preactivation}',
      `I _  3292- 3416 | , description={The preactivation of a neuron is the weighted sum of its input, before the activation function is applied}}␊
      ␊
      `,
      'E _  3416- 3455 | \\newglossaryentry{stroke}{name={stroke}',
      `I _  3455- 3581 | , description={The path the pen took from␊
      the point where the pen was put down to the point where the pen was lifted first}}␊
      ␊
      `,
      'E _  3581- 3616 | \\newglossaryentry{line}{name={line}',
      `I _  3616- 3702 | , description={Geometric object that is infinitely long␊
      and defined by two points.}}␊
      ␊
      `,
      'E _  3702- 3753 | \\newglossaryentry{line segment}{name={line segment}',
      `I _  3753- 3838 | , description={Geometric object that has finite length␊
      and defined by two points.}}␊
      ␊
      `,
      'E _  3838- 3877 | \\newglossaryentry{symbol}{name={symbol}',
      'I _  3877- 3963 | , description={An atomic semantic entity. A more detailed description can be found in ',
      'E _  3963- 3990 | \\cref{sec:what-is-a-symbol}',
      `I _  3990- 3994 | }}␊
      ␊
      `,
      'E _  3994- 4033 | \\newglossaryentry{weight}{name={weight}',
      `I _  4033- 4050 | , description={A␊
      `,
      'E _  4050- 4057 | \\textit',
      'I _  4057- 4128 | {weight} is a parameter of a neural net, that can be learned}, symbol={',
      'E _  4128- 4148 | \\ensuremath{\\weight}',
      `I _  4148- 4152 | }}␊
      ␊
      `,
      'E _  4152- 4205 | \\newglossaryentry{control point}{name={control point}',
      `I _  4205- 4222 | , description={A␊
      `,
      'E _  4222- 4229 | \\textit',
      'I _  4229- 4287 | {control point} is a point recorded by the input device.}}',
    ]

## verify samples/papers/write-math-paper/ch7-mfrdb-eval.tex against snapshot

> Snapshot 1

    [
      `I _     0-   38 | %!TEX root = write-math-ba-paper.tex␊
      ␊
      `,
      'E _    38-   46 | \\section',
      `I _    46-  138 | {Evaluation}␊
      ␊
      The optimized classifier was evaluated on three publicly available data sets:␊
      `,
      'E _   138-  143 | \\verb',
      'I _   143-  164 | +MfrDB_Symbols_v1.0+ ',
      'E _   164-  180 | \\cite{Stria2012}',
      'I _   180-  194 | , CROHME~2011 ',
      'E _   194-  213 | \\cite{Mouchere2011}',
      `I _   213-  231 | ,␊
      and CROHME~2012 `,
      'E _   231-  250 | \\cite{Mouchere2012}',
      `I _   250-  253 | .␊
      ␊
      `,
      'E _   253-  258 | \\verb',
      `I _   258-  494 | +MfrDB_Symbols_v1.0+ contains recordings for 105~symbols, but for␊
      11~symbols less than 50~recordings were available. For this reason, the␊
      optimized classifier was evaluated on 94~of the 105~symbols.␊
      ␊
      The evaluation results are given in `,
      'E _   494-  526 | \\cref{table:public-eval-results}',
      `I _   526-  529 | .␊
      ␊
      `,
      'E _   529-  547 | \\begin{table}[htb]',
      `I _   547-  552 | ␊
          `,
      'E _   552-  562 | \\centering',
      `I _   562-  567 | ␊
          `,
      'E _   567-  588 | \\begin{tabular}{lcrr}',
      `I _   588-  593 | ␊
          `,
      'E _   593-  601 | \\toprule',
      `I _   601-  606 | ␊
          `,
      'E _   606-  630 | \\multirow{2}{*}{Dataset}',
      'I _   630-  634 |   & ',
      'E _   634-  658 | \\multirow{2}{*}{Symbols}',
      'I _   658-  662 |   & ',
      'E _   662-  702 | \\multicolumn{2}{c}{Classification error}',
      `I _   702-  709 | \\\\␊
          `,
      'E _   709-  718 | \\cmidrule',
      `I _   718-  824 | (l){3-4}␊
                    & & Top-1                 & Top-3                \\\\\\midrule␊
          MfrDB       & 94 & `,
      'E _   824-  844 | $\\SI{8.4}{\\percent}$',
      'I _   844-  848 |   & ',
      'E _   848-  868 | $\\SI{1.3}{\\percent}$',
      `I _   868-  895 |  \\\\␊
          CROHME 2011 & 56 & `,
      'E _   895-  916 | $\\SI{10.2}{\\percent}$',
      'I _   916-  919 |  & ',
      'E _   919-  939 | $\\SI{3.7}{\\percent}$',
      `I _   939-  966 |  \\\\␊
          CROHME 2012 & 75 & `,
      'E _   966-  987 | $\\SI{12.2}{\\percent}$',
      'I _   987-  990 |  & ',
      'E _   990- 1010 | $\\SI{4.1}{\\percent}$',
      `I _  1010- 1018 |  \\\\␊
          `,
      'E _  1018- 1029 | \\bottomrule',
      `I _  1029- 1034 | ␊
          `,
      'E _  1034- 1047 | \\end{tabular}',
      `I _  1047- 1052 | ␊
          `,
      'E _  1052- 1060 | \\caption',
      `I _  1060- 1324 | {Error rates of the optimized recognizer systems. The systems␊
                   output layer was adjusted to the number of symbols it should␊
                   recognize and trained with the combined data from␊
                   write-math and the training given by the datasets.}␊
      `,
      'E _  1324- 1357 | \\label{table:public-eval-results}',
      `I _  1357- 1358 | ␊
      `,
      'E _  1358- 1369 | \\end{table}',
    ]

## verify samples/papers/write-math-paper/ch6-summary.tex against snapshot

> Snapshot 1

    [
      `I _     0-   38 | %!TEX root = write-math-ba-paper.tex␊
      ␊
      `,
      'E _    38-   46 | \\section',
      `I _    46-  434 | {Summary}␊
      Four baseline recognition systems were adjusted in many experiments and their␊
      recognition capabilities were compared in order to build a recognition system␊
      that can recognize 396 mathematical symbols with low error rates as well as to␊
      evaluate which preprocessing steps and features help to improve the recognition␊
      rate.␊
      ␊
      All recognition systems were trained and evaluated with␊
      `,
      'E _   434-  469 | $\\num{\\totalCollectedRecordings{}}$',
      'I _   469-  485 |  recordings for ',
      'E _   485-  508 | \\totalClassesAnalyzed{}',
      `I _   508-  581 | ␊
      symbols. These recordings were collected by two crowdsourcing projects␊
      (`,
      'E _   581-  640 | \\href{http://detexify.kirelabs.org/classify.html}{Detexify}',
      `I _   640-  645 |  and␊
      `,
      'E _   645-  682 | \\href{write-math.com}{write-math.com}',
      `I _   682-  851 | ) and created with various devices. While␊
      some recordings were created with standard touch devices such as tablets and␊
      smartphones, others were created with the mouse.␊
      ␊
      `,
      'E _   851-  862 | \\Glspl{MLP}',
      `I _   862- 1049 |  were used for the classification task. Four baseline systems with␊
      different numbers of hidden layers were used, as the number of hidden layer␊
      influences the capabilities and problems of `,
      'E _  1049- 1060 | \\glspl{MLP}',
      `I _  1060- 1174 | .␊
      ␊
      All baseline systems used the same preprocessing queue. The recordings were␊
      scaled and shifted as described in `,
      'E _  1174- 1197 | \\ref{sec:preprocessing}',
      `I _  1197- 1324 | , resampled with␊
      linear interpolation so that every stroke had exactly 20~points which are␊
      spread equidistant in time. The 80~(`,
      'E _  1324- 1329 | $x,y$',
      `I _  1329- 1391 | ) coordinates of the first 4~strokes␊
      were used to get exactly `,
      'E _  1391- 1396 | $160$',
      `I _  1396- 1453 |  input features for every recording. The baseline␊
      system `,
      'E _  1453- 1463 | $B_{hl=2}$',
      'I _  1463- 1485 |  has a top-3 error of ',
      'E _  1485- 1505 | $\\SI{5.7}{\\percent}$',
      `I _  1505- 1617 | .␊
      ␊
      Adding two slightly rotated variants for each recording and hence tripling the␊
      training set made the systems `,
      'E _  1617- 1627 | $B_{hl=3}$',
      'I _  1627- 1632 |  and ',
      'E _  1632- 1642 | $B_{hl=4}$',
      `I _  1642- 1806 |  perform much worse, but␊
      improved the performance of the smaller systems.␊
      ␊
      The global features re-curvature, ink, stoke count and aspect ratio improved␊
      the systems `,
      'E _  1806- 1816 | $B_{hl=1}$',
      'I _  1816- 1818 | --',
      'E _  1818- 1828 | $B_{hl=3}$',
      `I _  1828- 1875 | , whereas the stroke center point feature␊
      made `,
      'E _  1875- 1885 | $B_{hl=2}$',
      `I _  1885- 2027 |  perform worse.␊
      ␊
      Denoising auto-encoders were evaluated as one way to use pretraining, but by␊
      this the error rate increased notably. However, `,
      'E _  2027- 2040 | \\acrlong{SLP}',
      `I _  2040- 2325 |  improved the␊
      performance decidedly.␊
      ␊
      The stroke connection algorithm was added to the preprocessing steps of the␊
      baseline system as well as the re-curvature feature, the ink feature, the␊
      number of strokes and the aspect ratio. The training setup of the baseline␊
      system was changed to `,
      'E _  2325- 2338 | \\acrlong{SLP}',
      `I _  2338- 2435 |  and the resulting model was trained with a␊
      lower learning rate again. This optimized recognizer `,
      'E _  2435- 2448 | $B_{hl=2,c}\'$',
      `I _  2448- 2470 |  had a top-3␊
      error of `,
      'E _  2470- 2490 | $\\SI{4.0}{\\percent}$',
      `I _  2490- 2540 | . This means that the top-3 error dropped by over␊
      `,
      'E _  2540- 2551 | $\\num{1.7}$',
      'I _  2551- 2607 |  percentage points in comparison to the baseline system ',
      'E _  2607- 2617 | $B_{hl=2}$',
      `I _  2617- 2637 | .␊
      ␊
      A top-3 error of `,
      'E _  2637- 2657 | $\\SI{4.0}{\\percent}$',
      `I _  2657- 3035 |  makes the system usable for symbol␊
      lookup. It could also be used as a starting point for the development of a␊
      multiple-symbol classifier.␊
      ␊
      The aim of this work was to develop a symbol recognition system which is easy␊
      to use, fast and has high recognition rates as well as evaluating ideas for␊
      single symbol classifiers. Some of those goals were reached. The recognition␊
      system `,
      'E _  3035- 3048 | $B_{hl=2,c}\'$',
      `I _  3048- 3300 |  evaluates new recordings in a fraction of a second and has␊
      acceptable recognition rates.␊
      ␊
      % Many algorithms were evaluated. However, there are still many other␊
      % algorithms which could be evaluated and, at the time of this work, the best␊
      % classifier `,
      'E _  3300- 3313 | $B_{hl=2,c}\'$',
      `I _  3313- 3361 |  is only available through the Python package␊
      % `,
      'E _  3361- 3368 | \\texttt',
      `I _  3368- 3440 | {hwrt}. It is planned to add an web version of that classifier online.␊
      ␊
      `,
      'E _  3440- 3448 | \\section',
      `I _  3448- 3722 | {Optimized Recognizer}␊
      All preprocessing steps and features that were useful were combined to create a␊
      recognizer that performs best.␊
      ␊
      All models were much better than everything that was tried before. The results␊
      of this experiment show that single-symbol recognition with␊
      `,
      'E _  3722- 3745 | \\totalClassesAnalyzed{}',
      `I _  3745- 3831 |  classes and usual touch devices and the mouse can be␊
      done with a top-1 error rate of `,
      'E _  3831- 3852 | $\\SI{18.6}{\\percent}$',
      `I _  3852- 3874 |  and a top-3 error of␊
      `,
      'E _  3874- 3894 | $\\SI{4.1}{\\percent}$',
      `I _  3894- 3919 | . This was␊
      achieved by a `,
      'E _  3919- 3928 | \\gls{MLP}',
      'I _  3928- 3936 |  with a ',
      'E _  3936- 3979 | $167{:}500{:}500{:}\\totalClassesAnalyzed{}$',
      `I _  3979- 4075 |  topology.␊
      ␊
      It used the stroke connection algorithm to connect of which the ends were less␊
      than `,
      'E _  4075- 4092 | $\\SI{10}{\\pixel}$',
      `I _  4092- 4166 |  away, scaled each recording to a unit square and shifted␊
      as described in `,
      'E _  4166- 4189 | \\ref{sec:preprocessing}',
      `I _  4189- 4334 | . After that, a linear resampling step␊
      was applied to the first 4 strokes to resample them to 20 points each. All␊
      other strokes were discarded.␊
      ␊
      `,
      'E _  4334- 4344 | \\goodbreak',
      `I _  4344- 4366 | ␊
      The 167 features were`,
      'E _  4366- 4379 | \\mynobreakpar',
      `I _  4379- 4381 | %␊
      `,
      'E _  4381- 4396 | \\begin{itemize}',
      `I _  4396- 4402 | ␊
           `,
      'E _  4402- 4407 | \\item',
      `I _  4407- 4497 |  the first 4 strokes with 20 points per stroke resulting in 160␊
                 features,␊
           `,
      'E _  4497- 4502 | \\item',
      `I _  4502- 4550 |  the re-curvature for the first 4 strokes,␊
           `,
      'E _  4550- 4555 | \\item',
      `I _  4555- 4570 |  the ink,␊
           `,
      'E _  4570- 4575 | \\item',
      `I _  4575- 4607 |  the number of strokes and␊
           `,
      'E _  4607- 4612 | \\item',
      `I _  4612- 4650 |  the aspect ratio of the bounding box␊
      `,
      'E _  4650- 4663 | \\end{itemize}',
      `I _  4663- 4665 | ␊
      ␊
      `,
      'E _  4665- 4674 | \\Gls{SLP}',
      'I _  4674- 4692 |  was applied with ',
      'E _  4692- 4704 | $\\num{1000}$',
      `I _  4704- 4742 |  epochs per layer, a␊
      learning rate of `,
      'E _  4742- 4752 | $\\eta=0.1$',
      'I _  4752- 4771 |  and a momentum of ',
      'E _  4771- 4783 | $\\alpha=0.1$',
      `I _  4783- 4838 | . After that, the␊
      complete model was trained again for `,
      'E _  4838- 4844 | $1000$',
      `I _  4844- 4915 |  epochs with standard mini-batch␊
      gradient descent resulting in systems `,
      'E _  4915- 4928 | $B_{hl=1,c}\'$',
      'I _  4928- 4932 |  -- ',
      'E _  4932- 4945 | $B_{hl=4,c}\'$',
      `I _  4945- 4965 | .␊
      ␊
      After the models `,
      'E _  4965- 4977 | $B_{hl=1,c}$',
      'I _  4977- 4981 |  -- ',
      'E _  4981- 4993 | $B_{hl=4,c}$',
      'I _  4993- 5017 |  were trained the first ',
      'E _  5017- 5023 | $1000$',
      `I _  5023- 5060 |  epochs,␊
      they were trained again for `,
      'E _  5060- 5072 | $\\num{1000}$',
      'I _  5072- 5104 |  epochs with a learning rate of ',
      `E _  5104- 5117 | $\\eta =␊
      0.05$`,
      'I _  5117- 5119 | . ',
      'E _  5119- 5169 | \\Cref{table:complex-recognizer-systems-evaluation}',
      `I _  5169- 5219 |  shows that␊
      this improved the classifiers again.␊
      ␊
      `,
      'E _  5219- 5237 | \\begin{table}[htb]',
      `I _  5237- 5242 | ␊
          `,
      'E _  5242- 5252 | \\centering',
      `I _  5252- 5257 | ␊
          `,
      'E _  5257- 5279 | \\begin{tabular}{lrrrr}',
      `I _  5279- 5284 | ␊
          `,
      'E _  5284- 5292 | \\toprule',
      `I _  5292- 5297 | ␊
          `,
      'E _  5297- 5320 | \\multirow{2}{*}{System}',
      'I _  5320- 5324 |   & ',
      'E _  5324- 5364 | \\multicolumn{4}{c}{Classification error}',
      `I _  5364- 5371 | \\\\␊
          `,
      'E _  5371- 5380 | \\cmidrule',
      `I _  5380- 5497 | (l){2-5}␊
                    & Top-1                 & Change                & Top-3                & Change\\\\\\midrule␊
          `,
      'E _  5497- 5509 | $B_{hl=1,c}$',
      'I _  5509- 5512 |  & ',
      'E _  5512- 5533 | $\\SI{21.0}{\\percent}$',
      'I _  5533- 5536 |  & ',
      'E _  5536- 5557 | $\\SI{-2.2}{\\percent}$',
      'I _  5557- 5560 |  & ',
      'E _  5560- 5580 | $\\SI{5.2}{\\percent}$',
      'I _  5580- 5583 |  & ',
      'E _  5583- 5604 | $\\SI{-1.5}{\\percent}$',
      `I _  5604- 5611 | \\\\␊
          `,
      'E _  5611- 5623 | $B_{hl=2,c}$',
      'I _  5623- 5626 |  & ',
      'E _  5626- 5647 | $\\SI{18.3}{\\percent}$',
      'I _  5647- 5650 |  & ',
      'E _  5650- 5671 | $\\SI{-3.3}{\\percent}$',
      'I _  5671- 5674 |  & ',
      'E _  5674- 5694 | $\\SI{4.1}{\\percent}$',
      'I _  5694- 5697 |  & ',
      'E _  5697- 5718 | $\\SI{-1.6}{\\percent}$',
      `I _  5718- 5725 | \\\\␊
          `,
      'E _  5725- 5737 | $B_{hl=3,c}$',
      'I _  5737- 5740 |  & ',
      'E _  5740- 5772 | \\underline{$\\SI{18.2}{\\percent}$',
      'I _  5772- 5776 | } & ',
      'E _  5776- 5797 | $\\SI{-3.7}{\\percent}$',
      'I _  5797- 5800 |  & ',
      'E _  5800- 5831 | \\underline{$\\SI{4.1}{\\percent}$',
      'I _  5831- 5835 | } & ',
      'E _  5835- 5856 | $\\SI{-1.6}{\\percent}$',
      `I _  5856- 5863 | \\\\␊
          `,
      'E _  5863- 5875 | $B_{hl=4,c}$',
      'I _  5875- 5878 |  & ',
      'E _  5878- 5899 | $\\SI{18.6}{\\percent}$',
      'I _  5899- 5902 |  & ',
      'E _  5902- 5923 | $\\SI{-5.3}{\\percent}$',
      'I _  5923- 5926 |  & ',
      'E _  5926- 5946 | $\\SI{4.3}{\\percent}$',
      'I _  5946- 5949 |  & ',
      'E _  5949- 5970 | $\\SI{-1.9}{\\percent}$',
      `I _  5970- 5985 | \\\\\\midrule␊
          `,
      'E _  5985- 5998 | $B_{hl=1,c}\'$',
      'I _  5998- 6001 |  & ',
      'E _  6001- 6022 | $\\SI{19.3}{\\percent}$',
      'I _  6022- 6025 |  & ',
      'E _  6025- 6046 | $\\SI{-3.9}{\\percent}$',
      'I _  6046- 6049 |  & ',
      'E _  6049- 6069 | $\\SI{4.8}{\\percent}$',
      'I _  6069- 6072 |  & ',
      'E _  6072- 6093 | $\\SI{-1.9}{\\percent}$',
      `I _  6093- 6101 |  \\\\␊
          `,
      'E _  6101- 6114 | $B_{hl=2,c}\'$',
      'I _  6114- 6117 |  & ',
      'E _  6117- 6149 | \\underline{$\\SI{17.5}{\\percent}$',
      'I _  6149- 6153 | } & ',
      'E _  6153- 6174 | $\\SI{-4.1}{\\percent}$',
      'I _  6174- 6177 |  & ',
      'E _  6177- 6208 | \\underline{$\\SI{4.0}{\\percent}$',
      'I _  6208- 6212 | } & ',
      'E _  6212- 6233 | $\\SI{-1.7}{\\percent}$',
      `I _  6233- 6240 | \\\\␊
          `,
      'E _  6240- 6253 | $B_{hl=3,c}\'$',
      'I _  6253- 6256 |  & ',
      'E _  6256- 6277 | $\\SI{17.7}{\\percent}$',
      'I _  6277- 6280 |  & ',
      'E _  6280- 6301 | $\\SI{-4.2}{\\percent}$',
      'I _  6301- 6304 |  & ',
      'E _  6304- 6324 | $\\SI{4.1}{\\percent}$',
      'I _  6324- 6327 |  & ',
      'E _  6327- 6348 | $\\SI{-1.6}{\\percent}$',
      `I _  6348- 6355 | \\\\␊
          `,
      'E _  6355- 6368 | $B_{hl=4,c}\'$',
      'I _  6368- 6371 |  & ',
      'E _  6371- 6392 | $\\SI{17.8}{\\percent}$',
      'I _  6392- 6395 |  & ',
      'E _  6395- 6416 | $\\SI{-6.1}{\\percent}$',
      'I _  6416- 6419 |  & ',
      'E _  6419- 6439 | $\\SI{4.3}{\\percent}$',
      'I _  6439- 6442 |  & ',
      'E _  6442- 6463 | $\\SI{-1.9}{\\percent}$',
      `I _  6463- 6470 | \\\\␊
          `,
      'E _  6470- 6481 | \\bottomrule',
      `I _  6481- 6486 | ␊
          `,
      'E _  6486- 6499 | \\end{tabular}',
      `I _  6499- 6504 | ␊
          `,
      'E _  6504- 6512 | \\caption',
      `I _  6512- 6587 | {Error rates of the optimized recognizer systems. The systems␊
                   `,
      'E _  6587- 6600 | $B_{hl=i,c}\'$',
      'I _  6600- 6622 |  were trained another ',
      'E _  6622- 6634 | $\\num{1000}$',
      `I _  6634- 6679 |  epochs with a learning rate␊
                   of `,
      'E _  6679- 6690 | $\\eta=0.05$',
      `I _  6690- 6693 | .}␊
      `,
      'E _  6693- 6744 | \\label{table:complex-recognizer-systems-evaluation}',
      `I _  6744- 6745 | ␊
      `,
      'E _  6745- 6756 | \\end{table}',
      `I _  6756- 6757 | ␊
      `,
    ]

## verify samples/papers/write-math-paper/ch5-optimization-of-system-design.tex against snapshot

> Snapshot 1

    [
      `I _     0-   38 | %!TEX root = write-math-ba-paper.tex␊
      ␊
      `,
      'E _    38-   46 | \\section',
      'I _    46-   77 | {Optimization of System Design}',
      'E _    77-  117 | \\label{ch:Optimization-of-System-Design}',
      `I _   117-  221 | ␊
      In order to evaluate the effect of different preprocessing algorithms, features␊
      and adjustments in the `,
      'E _   221-  230 | \\gls{MLP}',
      `I _   230-  398 |  training and topology, the following baseline␊
      system was used:␊
      ␊
      Scale the recording to fit into a unit square while keeping the aspect ratio,␊
      shift it as described in `,
      'E _   398-  422 | \\cref{sec:preprocessing}',
      `I _   422-  826 | ,␊
      resample it with linear interpolation to get 20~points per stroke, spaced␊
      evenly in time. Take the first 4~strokes with 20~points per stroke and␊
      2~coordinates per point as features, resulting in 160~features which is equal␊
      to the number of input neurons. If a recording has less than 4~strokes, the␊
      remaining features were filled with zeroes.␊
      ␊
      All experiments were evaluated with four baseline systems `,
      'E _   826-  836 | $B_{hl=i}$',
      'I _   836-  838 | , ',
      `E _   838-  862 | $i \\in \\Set{1,␊
      2, 3, 4}$`,
      'I _   862-  870 | , where ',
      'E _   870-  873 | $i$',
      `I _   873- 1060 |  is the number of hidden layers as different topologies␊
      could have a severe influence on the effect of new features or preprocessing␊
      steps. Each hidden layer in all evaluated systems has `,
      'E _  1060- 1065 | $500$',
      `I _  1065- 1081 |  neurons.␊
      ␊
      Each `,
      'E _  1081- 1090 | \\gls{MLP}',
      'I _  1090- 1127 |  was trained with a learning rate of ',
      'E _  1127- 1139 | $\\eta = 0.1$',
      `I _  1139- 1158 |  and a momentum␊
      of `,
      'E _  1158- 1172 | $\\alpha = 0.1$',
      `I _  1172- 1393 | . The activation function of every neuron in a hidden layer is␊
      the sigmoid function. The neurons in the␊
      output layer use the softmax function. For every experiment, exactly one part␊
      of the baseline systems was changed.␊
      ␊
      ␊
      `,
      'E _  1393- 1404 | \\subsection',
      `I _  1404- 1528 | {Random Weight Initialization}␊
      The neural networks in all experiments got initialized with a small random␊
      weight␊
      ␊
      \\[w_{i,j} `,
      'E _  1528- 1532 | \\sim',
      'I _  1532- 1538 |  U(-4 ',
      'E _  1538- 1543 | \\cdot',
      'I _  1543- 1544 |  ',
      'E _  1544- 1572 | \\sqrt{\\frac{6}{n_l + n_{l+1}',
      'I _  1572- 1578 | }}, 4 ',
      'E _  1578- 1583 | \\cdot',
      'I _  1583- 1584 |  ',
      'E _  1584- 1612 | \\sqrt{\\frac{6}{n_l + n_{l+1}',
      `I _  1612- 1625 | }})\\]␊
      ␊
      where `,
      'E _  1625- 1634 | $w_{i,j}$',
      'I _  1634- 1669 |  is the weight between the neurons ',
      'E _  1669- 1672 | $i$',
      'I _  1672- 1677 |  and ',
      'E _  1677- 1680 | $j$',
      'I _  1680- 1682 | , ',
      'E _  1682- 1685 | $l$',
      `I _  1685- 1709 |  is the layer␊
      of neuron `,
      'E _  1709- 1712 | $i$',
      'I _  1712- 1718 | , and ',
      'E _  1718- 1723 | $n_i$',
      'I _  1723- 1758 |  is the number of neurons in layer ',
      'E _  1758- 1761 | $i$',
      `I _  1761- 1807 | . This random␊
      initialization was suggested on␊
      `,
      'E _  1807- 1833 | \\cite{deeplearningweights}',
      `I _  1833- 2152 |  and is done to break symmetry.␊
      ␊
      This can lead to different error rates for the same systems just because the␊
      initialization was different.␊
      ␊
      In order to get an impression of the magnitude of the influence on the different␊
      topologies and error rates the baseline models were trained 5 times with␊
      random initializations.␊
      `,
      'E _  2152- 2212 | \\Cref{table:baseline-systems-random-initializations-summary}',
      `I _  2212- 2357 | ␊
      shows a summary of the results. The more hidden layers are used, the more do␊
      the results vary between different random weight initializations.␊
      ␊
      `,
      'E _  2357- 2373 | \\begin{table}[h]',
      `I _  2373- 2378 | ␊
          `,
      'E _  2378- 2388 | \\centering',
      `I _  2388- 2393 | ␊
          `,
      'E _  2393- 2418 | \\begin{tabular}{crrr|rrr}',
      `I _  2418- 2434 |  %chktex 44␊
          `,
      'E _  2434- 2442 | \\toprule',
      `I _  2442- 2447 | ␊
          `,
      'E _  2447- 2470 | \\multirow{3}{*}{System}',
      'I _  2470- 2474 |   & ',
      'E _  2474- 2514 | \\multicolumn{6}{c}{Classification error}',
      `I _  2514- 2521 | \\\\␊
          `,
      'E _  2521- 2530 | \\cmidrule',
      `I _  2530- 2556 | (l){2-7}␊
                     & `,
      'E _  2556- 2581 | \\multicolumn{3}{c}{Top-1}',
      'I _  2581- 2586 |    & ',
      'E _  2586- 2611 | \\multicolumn{3}{c}{Top-3}',
      `I _  2611- 2768 | \\\\␊
                     & Min                   & Max                   & Mean                  & Min                  & Max                  & Mean\\\\\\midrule␊
          `,
      'E _  2768- 2778 | $B_{hl=1}$',
      'I _  2778- 2781 |  & ',
      'E _  2781- 2802 | $\\SI{23.1}{\\percent}$',
      'I _  2802- 2805 |  & ',
      'E _  2805- 2826 | $\\SI{23.4}{\\percent}$',
      'I _  2826- 2829 |  & ',
      'E _  2829- 2850 | $\\SI{23.2}{\\percent}$',
      'I _  2850- 2853 |  & ',
      'E _  2853- 2873 | $\\SI{6.7}{\\percent}$',
      'I _  2873- 2876 |  & ',
      'E _  2876- 2896 | $\\SI{6.8}{\\percent}$',
      'I _  2896- 2899 |  & ',
      'E _  2899- 2919 | $\\SI{6.7}{\\percent}$',
      `I _  2919- 2927 |  \\\\␊
          `,
      'E _  2927- 2937 | $B_{hl=2}$',
      'I _  2937- 2940 |  & ',
      'E _  2940- 2972 | \\underline{$\\SI{21.4}{\\percent}$',
      'I _  2972- 2976 | } & ',
      'E _  2976- 3008 | \\underline{$\\SI{21.8}{\\percent}$',
      'I _  3008- 3011 | }& ',
      'E _  3011- 3043 | \\underline{$\\SI{21.6}{\\percent}$',
      'I _  3043- 3047 | } & ',
      'E _  3047- 3067 | $\\SI{5.7}{\\percent}$',
      'I _  3067- 3070 |  & ',
      'E _  3070- 3101 | \\underline{$\\SI{5.8}{\\percent}$',
      'I _  3101- 3105 | } & ',
      'E _  3105- 3136 | \\underline{$\\SI{5.7}{\\percent}$',
      `I _  3136- 3144 | }\\\\␊
          `,
      'E _  3144- 3154 | $B_{hl=3}$',
      'I _  3154- 3157 |  & ',
      'E _  3157- 3178 | $\\SI{21.5}{\\percent}$',
      'I _  3178- 3181 |  & ',
      'E _  3181- 3202 | $\\SI{22.3}{\\percent}$',
      'I _  3202- 3205 |  & ',
      'E _  3205- 3226 | $\\SI{21.9}{\\percent}$',
      'I _  3226- 3229 |  & ',
      'E _  3229- 3260 | \\underline{$\\SI{5.5}{\\percent}$',
      'I _  3260- 3264 | } & ',
      'E _  3264- 3284 | $\\SI{5.8}{\\percent}$',
      'I _  3284- 3287 |  & ',
      'E _  3287- 3318 | \\underline{$\\SI{5.7}{\\percent}$',
      `I _  3318- 3326 | }\\\\␊
          `,
      'E _  3326- 3336 | $B_{hl=4}$',
      'I _  3336- 3339 |  & ',
      'E _  3339- 3360 | $\\SI{23.2}{\\percent}$',
      'I _  3360- 3363 |  & ',
      'E _  3363- 3384 | $\\SI{24.8}{\\percent}$',
      'I _  3384- 3387 |  & ',
      'E _  3387- 3408 | $\\SI{23.9}{\\percent}$',
      'I _  3408- 3411 |  & ',
      'E _  3411- 3431 | $\\SI{6.0}{\\percent}$',
      'I _  3431- 3434 |  & ',
      'E _  3434- 3454 | $\\SI{6.4}{\\percent}$',
      'I _  3454- 3457 |  & ',
      'E _  3457- 3477 | $\\SI{6.2}{\\percent}$',
      `I _  3477- 3484 | \\\\␊
          `,
      'E _  3484- 3495 | \\bottomrule',
      `I _  3495- 3500 | ␊
          `,
      'E _  3500- 3513 | \\end{tabular}',
      `I _  3513- 3518 | ␊
          `,
      'E _  3518- 3526 | \\caption',
      'I _  3526- 3539 | {The systems ',
      'E _  3539- 3549 | $B_{hl=1}$',
      'I _  3549- 3553 |  -- ',
      'E _  3553- 3563 | $B_{hl=4}$',
      `I _  3563- 3707 |  were randomly initialized,␊
                   trained and evaluated 5~times to estimate the influence of random␊
                   weight initialization.}␊
      `,
      'E _  3707- 3768 | \\label{table:baseline-systems-random-initializations-summary}',
      `I _  3768- 3769 | ␊
      `,
      'E _  3769- 3780 | \\end{table}',
      `I _  3780- 3783 | ␊
      ␊
      ␊
      `,
      'E _  3783- 3794 | \\subsection',
      `I _  3794- 3995 | {Stroke connection}␊
      In order to solve the problem of interrupted strokes, pairs of strokes␊
      can be connected with stroke connection algorithm. The idea is that for␊
      a pair of consecutively drawn strokes `,
      'E _  3995- 4011 | $s_{i}, s_{i+1}$',
      'I _  4011- 4027 |  the last point ',
      'E _  4027- 4032 | $s_i$',
      `I _  4032- 4064 |  is␊
      close to the first point of `,
      'E _  4064- 4073 | $s_{i+1}$',
      `I _  4073- 4128 |  if a stroke was accidentally split␊
      into two strokes.␊
      ␊
      `,
      'E _  4128- 4147 | $\\SI{59}{\\percent}$',
      `I _  4147- 4211 |  of all stroke pair distances in the collected data are␊
      between `,
      'E _  4211- 4228 | $\\SI{30}{\\pixel}$',
      'I _  4228- 4233 |  and ',
      'E _  4233- 4251 | $\\SI{150}{\\pixel}$',
      `I _  4251- 4310 | . Hence the stroke connection␊
      algorithm was evaluated with `,
      'E _  4310- 4326 | $\\SI{5}{\\pixel}$',
      'I _  4326- 4328 | , ',
      'E _  4328- 4345 | $\\SI{10}{\\pixel}$',
      `I _  4345- 4350 |  and␊
      `,
      'E _  4350- 4367 | $\\SI{20}{\\pixel}$',
      `I _  4367- 4421 | .␊
      All models top-3 error improved with a threshold of `,
      'E _  4421- 4447 | $\\theta = \\SI{10}{\\pixel}$',
      `I _  4447- 4460 | ␊
      by at least `,
      'E _  4460- 4471 | $\\num{0.2}$',
      'I _  4471- 4498 |  percentage points, except ',
      'E _  4498- 4508 | $B_{hl=4}$',
      `I _  4508- 4542 |  which did not notably␊
      improve.␊
      ␊
      ␊
      `,
      'E _  4542- 4553 | \\subsection',
      `I _  4553- 4643 | {Douglas-Peucker Smoothing}␊
      The Douglas-Peucker algorithm was applied with a threshold of `,
      `E _  4643- 4663 | $\\varepsilon =␊
      0.05$`,
      'I _  4663- 4665 | , ',
      'E _  4665- 4684 | $\\varepsilon = 0.1$',
      'I _  4684- 4689 |  and ',
      'E _  4689- 4708 | $\\varepsilon = 0.2$',
      `I _  4708- 5043 |  after scaling and shifting,␊
      but before resampling. The interpolation in the resampling step was done␊
      linearly and with cubic splines in two experiments. The recording was scaled␊
      and shifted again after the interpolation because the bounding box might have␊
      changed.␊
      ␊
      The result of the application of the Douglas-Peucker smoothing with `,
      `E _  5043- 5063 | $\\varepsilon␊
      > 0.05$`,
      'I _  5063- 5124 |  was a high rise of the top-1 and top-3 error for all models ',
      'E _  5124- 5134 | $B_{hl=i}$',
      `I _  5134- 5272 | .␊
      This means that the simplification process removes some relevant information and␊
      does not---as it was expected---remove only noise. For `,
      'E _  5272- 5292 | $\\varepsilon = 0.05$',
      `I _  5292- 5507 | ␊
      with linear interpolation some models top-1 error improved, but the␊
      changes were small. It could be an effect of random weight initialization.␊
      However, cubic spline interpolation made all systems perform more than␊
      `,
      'E _  5507- 5518 | $\\num{1.7}$',
      `I _  5518- 5594 |  percentage points worse for top-1 and top-3 error.␊
      ␊
      The lower the value of `,
      'E _  5594- 5607 | $\\varepsilon$',
      `I _  5607- 5791 | , the less does the recording change after␊
      this preprocessing step. As it was applied after scaling the recording such that␊
      the biggest dimension of the recording (width or height) is `,
      'E _  5791- 5794 | $1$',
      `I _  5794- 5807 | , a value of␊
      `,
      'E _  5807- 5827 | $\\varepsilon = 0.05$',
      'I _  5827- 5868 |  means that a point has to move at least ',
      'E _  5868- 5886 | $\\SI{5}{\\percent}$',
      `I _  5886- 5915 | ␊
      of the biggest dimension.␊
      ␊
      ␊
      `,
      'E _  5915- 5926 | \\subsection',
      `I _  5926- 6050 | {Global Features}␊
      Single global features were added one at a time to the baseline systems. Those␊
      features were re-curvature␊
      `,
      'E _  6050- 6133 | $\\text{re-curvature}(stroke) = \\frac{\\text{height}(stroke)}{\\text{length}(stroke)}$',
      `I _  6133- 6150 | ␊
      as described in `,
      'E _  6150- 6164 | \\cite{Huang06}',
      `I _  6164- 6369 | , the ink feature which is the summed length␊
      of all strokes, the stroke count, the aspect ratio and the stroke center points␊
      for the first four strokes. The stroke center point feature improved the system␊
      `,
      'E _  6369- 6379 | $B_{hl=1}$',
      'I _  6379- 6383 |  by ',
      'E _  6383- 6394 | $\\num{0.3}$',
      'I _  6394- 6444 | ~percentage points for the top-3 error and system ',
      'E _  6444- 6454 | $B_{hl=3}$',
      `I _  6454- 6478 |  for␊
      the top-1 error by `,
      'E _  6478- 6489 | $\\num{0.7}$',
      `I _  6489- 6643 | ~percentage points, but all other systems and␊
      error measures either got worse or did not improve much.␊
      ␊
      The other global features did improve the systems `,
      'E _  6643- 6653 | $B_{hl=1}$',
      'I _  6653- 6657 |  -- ',
      'E _  6657- 6667 | $B_{hl=3}$',
      `I _  6667- 6677 | , but not␊
      `,
      'E _  6677- 6687 | $B_{hl=4}$',
      `I _  6687- 6781 | . The highest improvement was achieved with the re-curvature feature. It␊
      improved the systems `,
      'E _  6781- 6791 | $B_{hl=1}$',
      'I _  6791- 6795 |  -- ',
      'E _  6795- 6805 | $B_{hl=4}$',
      'I _  6805- 6819 |  by more than ',
      'E _  6819- 6830 | $\\num{0.6}$',
      `I _  6830- 6864 | ~percentage points␊
      top-1 error.␊
      ␊
      ␊
      `,
      'E _  6864- 6875 | \\subsection',
      `I _  6875- 7335 | {Data Multiplication}␊
      Data multiplication can be used to make the model invariant to transformations.␊
      However, this idea seems not to work well in the domain of on-line handwritten␊
      mathematical symbols. We tripled the data by adding a version that is rotated␊
      3~degrees to the left and another one that is rotated 3~degrees to the right␊
      around the center of mass. This data multiplication made all classifiers for␊
      most error measures perform worse by more than `,
      'E _  7335- 7344 | $\\num{2}$',
      `I _  7344- 7679 | ~percentage points for␊
      the top-1 error.␊
      ␊
      The same experiment was executed by rotating by 6~degrees and in another␊
      experiment by 9~degrees, but those performed even worse.␊
      ␊
      Also multiplying the data by a factor of 5 by adding two 3-degree rotated␊
      variants and two 6-degree rotated variant made the classifier perform worse␊
      by more than `,
      'E _  7679- 7688 | $\\num{2}$',
      `I _  7688- 7710 | ~percentage points.␊
      ␊
      ␊
      `,
      'E _  7710- 7721 | \\subsection',
      'I _  7721- 7734 | {Pretraining}',
      'E _  7734- 7771 | \\label{subsec:pretraining-evaluation}',
      `I _  7771- 7831 | ␊
      Pretraining is a technique used to improve the training of `,
      'E _  7831- 7842 | \\glspl{MLP}',
      `I _  7842- 7873 |  with␊
      multiple hidden layers.␊
      ␊
      `,
      'E _  7873- 7901 | \\Cref{table:pretraining-slp}',
      'I _  7901- 7913 |  shows that ',
      'E _  7913- 7922 | \\gls{SLP}',
      `I _  7922- 7966 |  improves the classification␊
      performance by `,
      'E _  7966- 7977 | $\\num{1.6}$',
      `I _  7977- 8020 |  percentage points for the top-1 error and␊
      `,
      'E _  8020- 8031 | $\\num{1.0}$',
      `I _  8031- 8089 |  percentage points for the top-3 error. As one can see in␊
      `,
      'E _  8089- 8160 | \\cref{fig:training-and-test-error-for-different-topologies-pretraining}',
      `I _  8160- 8264 | , this␊
      is not only the case because of the longer training as the test error is␊
      relatively stable after `,
      'E _  8264- 8276 | $\\num{1000}$',
      `I _  8276- 8378 |  epochs of training. This was confirmed␊
      by an experiment where the baseline systems where trained for `,
      'E _  8378- 8391 | $\\num{10000}$',
      `I _  8391- 8439 | ␊
      epochs and did not perform notably different.␊
      ␊
      `,
      'E _  8439- 8458 | \\begin{figure}[htb]',
      `I _  8458- 8463 | ␊
          `,
      'E _  8463- 8473 | \\centering',
      `I _  8473- 8478 | ␊
          `,
      'E _  8478- 8553 | \\input{figures/errors-by-epoch-pretraining/errors-by-epoch-pretraining.tex}',
      `I _  8553- 8558 | ␊
          `,
      'E _  8558- 8566 | \\caption',
      `I _  8566- 8663 | {Training- and test error by number of trained epochs for different␊
                   topologies with `,
      'E _  8663- 8676 | \\acrfull{SLP}',
      `I _  8676- 8887 | . The plot shows␊
                   that all pretrained systems performed much better than the systems␊
                   without pretraining. All plotted systems did not improve␊
                   with more epochs of training.}␊
      `,
      'E _  8887- 8959 | \\label{fig:training-and-test-error-for-different-topologies-pretraining}',
      `I _  8959- 8960 | ␊
      `,
      'E _  8960- 8972 | \\end{figure}',
      `I _  8972- 8974 | ␊
      ␊
      `,
      'E _  8974- 8991 | \\begin{table}[tb]',
      `I _  8991- 8996 | ␊
          `,
      'E _  8996- 9006 | \\centering',
      `I _  9006- 9011 | ␊
          `,
      'E _  9011- 9033 | \\begin{tabular}{lrrrr}',
      `I _  9033- 9038 | ␊
          `,
      'E _  9038- 9046 | \\toprule',
      `I _  9046- 9051 | ␊
          `,
      'E _  9051- 9074 | \\multirow{2}{*}{System}',
      'I _  9074- 9078 |   & ',
      'E _  9078- 9118 | \\multicolumn{4}{c}{Classification error}',
      `I _  9118- 9125 | \\\\␊
          `,
      'E _  9125- 9134 | \\cmidrule',
      `I _  9134- 9270 | (l){2-5}␊
                      & Top-1                  & Change               & Top-3                & Change                 \\\\\\midrule␊
          `,
      'E _  9270- 9280 | $B_{hl=1}$',
      'I _  9280- 9287 |      & ',
      'E _  9287- 9308 | $\\SI{23.2}{\\percent}$',
      'I _  9308- 9335 |   & -                    & ',
      'E _  9335- 9355 | $\\SI{6.7}{\\percent}$',
      `I _  9355- 9367 |  & - \\\\␊
          `,
      'E _  9367- 9381 | $B_{hl=2,SLP}$',
      'I _  9381- 9384 |  & ',
      'E _  9384- 9405 | $\\SI{19.9}{\\percent}$',
      'I _  9405- 9408 |  & ',
      'E _  9408- 9429 | $\\SI{-1.7}{\\percent}$',
      'I _  9429- 9432 |  & ',
      'E _  9432- 9452 | $\\SI{4.7}{\\percent}$',
      'I _  9452- 9455 |  & ',
      'E _  9455- 9476 | $\\SI{-1.0}{\\percent}$',
      `I _  9476- 9483 | \\\\␊
          `,
      'E _  9483- 9497 | $B_{hl=3,SLP}$',
      'I _  9497- 9500 |  & ',
      'E _  9500- 9532 | \\underline{$\\SI{19.4}{\\percent}$',
      'I _  9532- 9536 | } & ',
      'E _  9536- 9557 | $\\SI{-2.5}{\\percent}$',
      'I _  9557- 9560 |  & ',
      'E _  9560- 9591 | \\underline{$\\SI{4.6}{\\percent}$',
      'I _  9591- 9595 | } & ',
      'E _  9595- 9616 | $\\SI{-1.1}{\\percent}$',
      `I _  9616- 9623 | \\\\␊
          `,
      'E _  9623- 9637 | $B_{hl=4,SLP}$',
      'I _  9637- 9640 |  & ',
      'E _  9640- 9661 | $\\SI{19.6}{\\percent}$',
      'I _  9661- 9664 |  & ',
      'E _  9664- 9685 | $\\SI{-4.3}{\\percent}$',
      'I _  9685- 9688 |  & ',
      'E _  9688- 9719 | \\underline{$\\SI{4.6}{\\percent}$',
      'I _  9719- 9723 | } & ',
      'E _  9723- 9744 | $\\SI{-1.6}{\\percent}$',
      `I _  9744- 9751 | \\\\␊
          `,
      'E _  9751- 9762 | \\bottomrule',
      `I _  9762- 9767 | ␊
          `,
      'E _  9767- 9780 | \\end{tabular}',
      `I _  9780- 9785 | ␊
          `,
      'E _  9785- 9793 | \\caption',
      'I _  9793- 9837 | {Systems with 1--4 hidden layers which used ',
      'E _  9837- 9850 | \\acrfull{SLP}',
      `I _  9850- 9896 | ␊
                   compared to the mean of systems `,
      'E _  9896- 9906 | $B_{hl=1}$',
      'I _  9906- 9908 | --',
      'E _  9908- 9918 | $B_{hl=4}$',
      `I _  9918- 9945 |  displayed␊
                   in `,
      'E _  9945-10005 | \\cref{table:baseline-systems-random-initializations-summary}',
      `I _ 10005-10057 | ␊
                   which used pure gradient descent. The `,
      'E _ 10057-10066 | \\gls{SLP}',
      `I _ 10066-10114 | ␊
                   systems clearly performed worse.}␊
      `,
      'E _ 10114-10143 | \\label{table:pretraining-slp}',
      `I _ 10143-10144 | ␊
      `,
      'E _ 10144-10155 | \\end{table}',
      `I _ 10155-10239 | ␊
      ␊
      ␊
      Pretraining with denoising auto-encoder lead to the much worse results listed in␊
      `,
      'E _ 10239-10286 | \\cref{table:pretraining-denoising-auto-encoder}',
      'I _ 10286-10311 | . The first layer used a ',
      'E _ 10311-10318 | $\\tanh$',
      `I _ 10318-10368 | ␊
      activation function. Every layer was trained for `,
      'E _ 10368-10374 | $1000$',
      `I _ 10374-10390 |  epochs and the␊
      `,
      'E _ 10390-10399 | \\gls{MSE}',
      'I _ 10399-10434 |  loss function. A learning-rate of ',
      'E _ 10434-10448 | $\\eta = 0.001$',
      `I _ 10448-10466 | , a corruption of␊
      `,
      'E _ 10466-10483 | $\\varkappa = 0.3$',
      'I _ 10483-10490 |  and a ',
      'E _ 10490-10495 | $L_2$',
      'I _ 10495-10514 |  regularization of ',
      'E _ 10514-10533 | $\\lambda = 10^{-4}$',
      `I _ 10533-10632 |  were␊
      chosen. This pretraining setup made all systems with all error measures perform␊
      much worse.␊
      ␊
      `,
      'E _ 10632-10649 | \\begin{table}[tb]',
      `I _ 10649-10654 | ␊
          `,
      'E _ 10654-10664 | \\centering',
      `I _ 10664-10669 | ␊
          `,
      'E _ 10669-10691 | \\begin{tabular}{lrrrr}',
      `I _ 10691-10696 | ␊
          `,
      'E _ 10696-10704 | \\toprule',
      `I _ 10704-10709 | ␊
          `,
      'E _ 10709-10732 | \\multirow{2}{*}{System}',
      'I _ 10732-10736 |   & ',
      'E _ 10736-10776 | \\multicolumn{4}{c}{Classification error}',
      `I _ 10776-10783 | \\\\␊
          `,
      'E _ 10783-10792 | \\cmidrule',
      `I _ 10792-10929 | (l){2-5}␊
                       & Top-1                  & Change               & Top-3                & Change                 \\\\\\midrule␊
          `,
      'E _ 10929-10943 | $B_{hl=1,AEP}$',
      'I _ 10943-10946 |  & ',
      'E _ 10946-10967 | $\\SI{23.8}{\\percent}$',
      'I _ 10967-10970 |  & ',
      'E _ 10970-10991 | $\\SI{+0.6}{\\percent}$',
      'I _ 10991-10994 |  & ',
      'E _ 10994-11014 | $\\SI{7.2}{\\percent}$',
      'I _ 11014-11017 |  & ',
      'E _ 11017-11038 | $\\SI{+0.5}{\\percent}$',
      `I _ 11038-11045 | \\\\␊
          `,
      'E _ 11045-11059 | $B_{hl=2,AEP}$',
      'I _ 11059-11062 |  & ',
      'E _ 11062-11094 | \\underline{$\\SI{22.8}{\\percent}$',
      'I _ 11094-11098 | } & ',
      'E _ 11098-11119 | $\\SI{+1.2}{\\percent}$',
      'I _ 11119-11122 |  & ',
      'E _ 11122-11142 | $\\SI{6.4}{\\percent}$',
      'I _ 11142-11145 |  & ',
      'E _ 11145-11166 | $\\SI{+0.7}{\\percent}$',
      `I _ 11166-11173 | \\\\␊
          `,
      'E _ 11173-11187 | $B_{hl=3,AEP}$',
      'I _ 11187-11190 |  & ',
      'E _ 11190-11211 | $\\SI{23.1}{\\percent}$',
      'I _ 11211-11214 |  & ',
      'E _ 11214-11235 | $\\SI{+1.2}{\\percent}$',
      'I _ 11235-11238 |  & ',
      'E _ 11238-11269 | \\underline{$\\SI{6.1}{\\percent}$',
      'I _ 11269-11273 | } & ',
      'E _ 11273-11294 | $\\SI{+0.4}{\\percent}$',
      `I _ 11294-11301 | \\\\␊
          `,
      'E _ 11301-11315 | $B_{hl=4,AEP}$',
      'I _ 11315-11318 |  & ',
      'E _ 11318-11339 | $\\SI{25.6}{\\percent}$',
      'I _ 11339-11342 |  & ',
      'E _ 11342-11363 | $\\SI{+1.7}{\\percent}$',
      'I _ 11363-11366 |  & ',
      'E _ 11366-11386 | $\\SI{7.0}{\\percent}$',
      'I _ 11386-11389 |  & ',
      'E _ 11389-11410 | $\\SI{+0.8}{\\percent}$',
      `I _ 11410-11417 | \\\\␊
          `,
      'E _ 11417-11428 | \\bottomrule',
      `I _ 11428-11433 | ␊
          `,
      'E _ 11433-11446 | \\end{tabular}',
      `I _ 11446-11451 | ␊
          `,
      'E _ 11451-11459 | \\caption',
      'I _ 11459-11483 | {Systems with denoising ',
      'E _ 11483-11496 | \\acrfull{AEP}',
      `I _ 11496-11549 |  compared to pure␊
                   gradient descent. The `,
      'E _ 11549-11558 | \\gls{AEP}',
      `I _ 11558-11585 |  systems performed worse.}␊
      `,
      'E _ 11585-11633 | \\label{table:pretraining-denoising-auto-encoder}',
      `I _ 11633-11634 | ␊
      `,
      'E _ 11634-11645 | \\end{table}',
      `I _ 11645-11646 | ␊
      `,
    ]

## verify samples/papers/write-math-paper/ch4-algorithms.tex against snapshot

> Snapshot 1

    [
      `I _     0-   38 | %!TEX root = write-math-ba-paper.tex␊
      ␊
      `,
      'E _    38-   46 | \\section',
      `I _    46-   59 | {Algorithms}␊
      `,
      'E _    59-   70 | \\subsection',
      'I _    70-   85 | {Preprocessing}',
      'E _    85-  110 | \\label{sec:preprocessing}',
      `I _   110-  600 | ␊
      Preprocessing in symbol recognition is done to improve the quality and␊
      expressive power of the data. It makes follow-up tasks like feature extraction␊
      and classification easier, more effective or faster. It does so by resolving␊
      errors in the input data, reducing duplicate information and removing␊
      irrelevant information.␊
      ␊
      Preprocessing algorithms fall into two groups: Normalization and noise␊
      reduction algorithms.␊
      ␊
      A very important normalization algorithm in single-symbol recognition is␊
      `,
      'E _   600-  607 | \\textit',
      'I _   607-  625 | {scale-and-shift}~',
      'E _   625-  642 | \\cite{Thoma:2014}',
      `I _   642-  933 | . It scales the recording so that␊
      its bounding box fits into a unit square. As the aspect ratio of a recording is␊
      almost never 1:1, only one dimension will fit exactly in the unit square. For␊
      this paper, it was chosen to shift the recording in the direction of its bigger␊
      dimension into the `,
      'E _   933-  953 | $[0,1] \\times [0,1]$',
      `I _   953- 1155 |  unit square. After that, the recording␊
      is shifted in direction of its smaller dimension such that its bounding box is␊
      centered around zero.␊
      ␊
      Another normalization preprocessing algorithm is␊
      resampling~`,
      'E _  1155- 1177 | \\cite{Guyon91,Manke01}',
      `I _  1177- 1477 | . As the data points on the pen trajectory are␊
      generated asynchronously and with different time-resolutions depending on the␊
      used hardware and software, it is desirable to resample the recordings to have␊
      points spread equally in time for every recording. This was done by linear␊
      interpolation of the `,
      'E _  1477- 1484 | $(x,t)$',
      'I _  1484- 1489 |  and ',
      'E _  1489- 1496 | $(y,t)$',
      `I _  1496- 1572 |  sequences and getting a fixed number␊
      of equally spaced points per stroke.␊
      ␊
      `,
      'E _  1572- 1579 | \\textit',
      `I _  1579- 1652 | {Stroke connection} is a noise reduction algorithm which is mentioned␊
      in~`,
      'E _  1652- 1668 | \\cite{Tappert90}',
      `I _  1668- 2758 | . It happens sometimes that the hardware detects that the␊
      user lifted the pen where the user certainly didn't do so. This can be detected␊
      by measuring the Euclidean distance between the end of one stroke and the␊
      beginning of the next stroke. If this distance is below a threshold, then the␊
      strokes are connected.␊
      ␊
      Due to a limited resolution of the recording device and due to erratic␊
      handwriting, the pen trajectory might not be smooth. One way to smooth is␊
      calculating a weighted average and replacing points by the weighted average of␊
      their coordinate and their neighbors coordinates. Another way to do smoothing␊
      is to reduce the number of points with the Douglas-Peucker␊
      algorithm to the points that are more relevant for the␊
      overall shape of a stroke and then interpolate the stroke between those points.␊
      The Douglas-Peucker stroke simplification algorithm is usually used in␊
      cartography to simplify the shape of roads. It works recursively to find a␊
      subset of points of a stroke that is simpler and still similar to the original␊
      shape. The algorithm adds the first and the last point `,
      'E _  2758- 2763 | $p_1$',
      'I _  2763- 2768 |  and ',
      'E _  2768- 2773 | $p_n$',
      `I _  2773- 2818 |  of a␊
      stroke to the simplified set of points `,
      'E _  2818- 2821 | $S$',
      'I _  2821- 2850 | . Then it searches the point ',
      'E _  2850- 2855 | $p_i$',
      `I _  2855- 2907 |  in␊
      between that has maximum distance from the line `,
      'E _  2907- 2916 | $p_1 p_n$',
      `I _  2916- 2956 | . If this distance is␊
      above a threshold `,
      'E _  2956- 2969 | $\\varepsilon$',
      'I _  2969- 2981 | , the point ',
      'E _  2981- 2986 | $p_i$',
      'I _  2986- 2999 |  is added to ',
      'E _  2999- 3002 | $S$',
      `I _  3002- 3039 | . Then the␊
      algorithm gets applied to `,
      'E _  3039- 3048 | $p_1 p_i$',
      'I _  3048- 3053 |  and ',
      'E _  3053- 3062 | $p_i p_n$',
      `I _  3062- 3095 |  recursively. It is described␊
      as `,
      'E _  3095- 3103 | \\enquote',
      'I _  3103- 3120 | {Algorithm 1} in~',
      'E _  3120- 3142 | \\cite{Visvalingam1990}',
      `I _  3142- 3145 | .␊
      ␊
      `,
      'E _  3145- 3156 | \\subsection',
      'I _  3156- 3166 | {Features}',
      'E _  3166- 3186 | \\label{sec:features}',
      `I _  3186- 3203 | ␊
      Features can be `,
      'E _  3203- 3210 | \\textit',
      `I _  3210- 3375 | {global}, that means calculated for the complete␊
      recording or complete strokes. Other features are calculated for single points␊
      on the pen trajectory and are called `,
      'E _  3375- 3382 | \\textit',
      `I _  3382- 3416 | {local}.␊
      ␊
      Global features are the `,
      'E _  3416- 3423 | \\textit',
      `I _  3423- 3463 | {number of strokes} in a recording, the␊
      `,
      'E _  3463- 3470 | \\textit',
      `I _  3470- 3521 | {aspect ratio} of a recordings bounding box or the␊
      `,
      'E _  3521- 3528 | \\textit',
      `I _  3528- 3685 | {ink} being used for a recording. The ink feature gets calculated by␊
      measuring the length of all strokes combined. The re-curvature, which was␊
      introduced in~`,
      'E _  3685- 3699 | \\cite{Huang06}',
      `I _  3699- 3717 | , is defined as␊
      \\[`,
      'E _  3717- 3722 | \\text',
      'I _  3722- 3748 | {re-curvature}(stroke) := ',
      'E _  3748- 3767 | \\frac{\\text{height}',
      'I _  3767- 3777 | (stroke)}{',
      'E _  3777- 3782 | \\text',
      `I _  3782- 4020 | {length}(stroke)}\\]␊
      and a stroke-global feature.␊
      ␊
      The simplest local feature is the coordinate of the point itself. Speed,␊
      curvature and a local small-resolution bitmap around the point, which was␊
      introduced by Manke, Finke and Waibel in~`,
      'E _  4020- 4036 | \\cite{Manke1995}',
      `I _  4036- 4065 | , are other local␊
      features.␊
      ␊
      `,
      'E _  4065- 4076 | \\subsection',
      'I _  4076- 4100 | {Multilayer Perceptrons}',
      'E _  4100- 4124 | \\label{sec:mlp-training}',
      `I _  4124- 4125 | ␊
      `,
      'E _  4125- 4136 | \\Glspl{MLP}',
      'I _  4136- 4164 |  are explained in detail in~',
      'E _  4164- 4181 | \\cite{Mitchell97}',
      `I _  4181- 4370 | . They can have␊
      different numbers of hidden layers, the number of neurons per layer and the␊
      activation functions can be varied. The learning algorithm is parameterized by␊
      the learning rate `,
      'E _  4370- 4392 | $\\eta \\in (0, \\infty)$',
      'I _  4392- 4407 | , the momentum ',
      'E _  4407- 4431 | $\\alpha \\in [0, \\infty)$',
      `I _  4431- 4475 | ␊
      and the number of epochs.␊
      ␊
      The topology of `,
      'E _  4475- 4486 | \\glspl{MLP}',
      `I _  4486- 4605 |  will be denoted in the following by separating the␊
      number of neurons per layer with colons. For example, the notation␊
      `,
      'E _  4605- 4634 | $160{:}500{:}500{:}500{:}369$',
      `I _  4634- 4778 |  means that the input layer gets 160~features,␊
      there are three hidden layers with 500~neurons per layer and one output layer␊
      with 369~neurons.␊
      ␊
      `,
      'E _  4778- 4789 | \\glspl{MLP}',
      `I _  4789- 4859 |  training can be executed in various different ways, for example␊
      with `,
      'E _  4859- 4872 | \\acrfull{SLP}',
      'I _  4872- 4887 | . In case of a ',
      'E _  4887- 4896 | \\gls{MLP}',
      `I _  4896- 4915 |  with the topology␊
      `,
      'E _  4915- 4944 | $160{:}500{:}500{:}500{:}369$',
      'I _  4944- 4946 | , ',
      'E _  4946- 4955 | \\gls{SLP}',
      'I _  4955- 4985 |  works as follows: At first a ',
      'E _  4985- 4994 | \\gls{MLP}',
      `I _  4994- 5018 | ␊
      with one hidden layer (`,
      'E _  5018- 5035 | $160{:}500{:}369$',
      `I _  5035- 5176 | ) is trained. Then the output layer is␊
      discarded, a new hidden layer and a new output layer is added and it is trained␊
      again, resulting in a `,
      'E _  5176- 5199 | $160{:}500{:}500{:}369$',
      'I _  5199- 5200 |  ',
      'E _  5200- 5209 | \\gls{MLP}',
      `I _  5209- 5404 | . The output layer is␊
      discarded again, a new hidden layer is added and a new output layer is added␊
      and the training is executed again.␊
      ␊
      Denoising auto-encoders are another way of pretraining. An␊
      `,
      'E _  5404- 5411 | \\textit',
      `I _  5411- 5587 | {auto-encoder} is a neural network that is trained to restore its input.␊
      This means the number of input neurons is equal to the number of output␊
      neurons. The weights define an `,
      'E _  5587- 5594 | \\textit',
      `I _  5594- 5827 | {encoding} of the input that allows␊
      restoring the input. As the neural network finds the encoding by itself, it is␊
      called auto-encoder. If the hidden layer is smaller than the input layer, it␊
      can be used for dimensionality reduction~`,
      'E _  5827- 5844 | \\cite{Hinton1989}',
      `I _  5844- 5984 | . If only one hidden␊
      layer with linear activation functions is used, then the hidden layer contains␊
      the principal components after training~`,
      'E _  5984- 5999 | \\cite{Duda2001}',
      `I _  5999- 6054 | .␊
      ␊
      Denoising auto-encoders are a variant introduced in~`,
      'E _  6054- 6072 | \\cite{Vincent2008}',
      `I _  6072- 6332 |  that␊
      is more robust to partial corruption of the input features. It is trained to␊
      get robust by adding noise to the input features.␊
      ␊
      There are multiple ways how noise can be added. Gaussian noise and randomly␊
      masking elements with zero are two possibilities.␊
      `,
      'E _  6332- 6364 | \\cite{Deeplearning-Denoising-AE}',
      `I _  6364- 6463 |  describes how such a denoising auto-encoder␊
      with masking noise can be implemented. The corruption `,
      'E _  6463- 6485 | $\\varkappa \\in [0, 1)$',
      `I _  6485- 6531 |  is␊
      the probability of a feature being masked.`,
    ]

## verify samples/papers/write-math-paper/ch3-data-and-implementation.tex against snapshot

> Snapshot 1

    [
      `I _     0-   38 | %!TEX root = write-math-ba-paper.tex␊
      ␊
      `,
      'E _    38-   46 | \\section',
      `I _    46-   80 | {Data and Implementation}␊
      We used `,
      'E _    80-   91 | $\\num{369}$',
      'I _    91-  123 |  symbol classes with a total of ',
      'E _   123-  137 | $\\num{166898}$',
      `I _   137-  182 |  labeled␊
      recordings. Each class has at least `,
      'E _   182-  192 | $\\num{50}$',
      `I _   192-  222 |  labeled recordings, but over␊
      `,
      'E _   222-  227 | $200$',
      'I _   227-  251 |  symbols have more than ',
      'E _   251-  262 | $\\num{200}$',
      'I _   262-  291 |  labeled recordings and over ',
      'E _   291-  296 | $100$',
      `I _   296-  320 | ␊
      symbols have more than `,
      'E _   320-  325 | $500$',
      `I _   325-  414 |  labeled recordings.␊
      The data was collected by two crowd-sourcing projects (Detexify and␊
      `,
      'E _   414-  459 | \\href{http://write-math.com}{write-math.com})',
      `I _   459-  616 |  where users wrote␊
      symbols, were then given a list ordered by an early classification system and␊
      clicked on the symbol they wrote.␊
      ␊
      The data of Detexify and `,
      'E _   616-  660 | \\href{http://write-math.com}{write-math.com}',
      `I _   660-  729 |  was␊
      combined, filtered semi-automatically and can be downloaded via␊
      `,
      'E _   729-  783 | \\href{http://write-math.com/data}{write-math.com/data}',
      `I _   783-  987 |  as a compressed tar␊
      archive of CSV files.␊
      ␊
      All of the following preprocessing and feature computation algorithms were␊
      implemented and are publicly available as open-source software in the Python␊
      package `,
      'E _   987-  994 | \\texttt',
      `I _   994- 1005 | {hwrt}.␊
      ␊
      % `,
      'E _  1005- 1023 | cspell:ignore hwrt',
      `I _  1023- 1024 | ␊
      `,
    ]

## verify samples/papers/write-math-paper/ch2-general-system-design.tex against snapshot

> Snapshot 1

    [
      `I _     0-   38 | %!TEX root = write-math-ba-paper.tex␊
      ␊
      `,
      'E _    38-   46 | \\section',
      `I _    46-  125 | {General System Design}␊
      The following steps are used for symbol classification:`,
      'E _   125-  133 | \\nobreak',
      `I _   133-  134 | ␊
      `,
      'E _   134-  151 | \\begin{enumerate}',
      `I _   151-  156 | ␊
          `,
      'E _   156-  161 | \\item',
      'I _   161-  162 |  ',
      'E _   162-  169 | \\textbf',
      `I _   169-  771 | {Preprocessing}: Recorded data is never perfect. Devices have␊
                errors and people make mistakes while using the devices. To tackle␊
                these problems there are preprocessing algorithms to clean the data.␊
                The preprocessing algorithms can also remove unnecessary variations␊
                of the data that do not help in the classification process, but hide␊
                what is important. Having slightly different sizes of the same symbol␊
                is an example of such a variation. Four preprocessing algorithms that␊
                clean or normalize recordings are explained in␊
                `,
      'E _   771-  795 | \\cref{sec:preprocessing}',
      `I _   795-  801 | .␊
          `,
      'E _   801-  806 | \\item',
      'I _   806-  807 |  ',
      'E _   807-  814 | \\textbf',
      `I _   814- 1175 | {Data multiplication}: Learning systems need lots of data␊
                to learn internal parameters. If there is not enough data available,␊
                domain knowledge can be considered to create new artificial data from␊
                the original data. In the domain of on-line handwriting recognition,␊
                data can be multiplied by adding rotated variants.␊
          `,
      'E _  1175- 1180 | \\item',
      'I _  1180- 1181 |  ',
      'E _  1181- 1188 | \\textbf',
      `I _  1188- 1732 | {Feature extraction}: A feature is high-level information␊
                derived from the raw data after preprocessing. Some systems like␊
                Detexify take the result of the preprocessing step, but many compute␊
                new features. Those features can be designed by a human engineer or␊
                learned. Non-raw data features have the advantage that less␊
                training data is needed since the developer uses knowledge about␊
                handwriting to compute highly discriminative features. Various␊
                features are explained in `,
      'E _  1732- 1751 | \\cref{sec:features}',
      `I _  1751- 1753 | .␊
      `,
      'E _  1753- 1768 | \\end{enumerate}',
      `I _  1768- 1992 | ␊
      ␊
      After these steps, it is a classification task for which the classifier has to␊
      learn internal parameters before it can classify new recordings.We classified␊
      recordings by computing constant-sized feature vectors and using␊
      `,
      'E _  1992- 2003 | \\glspl{MLP}',
      'I _  2003- 2035 | . There are many ways to adjust ',
      'E _  2035- 2046 | \\glspl{MLP}',
      `I _  2046- 2192 |  (number of neurons and␊
      layers, activation functions) and their training (learning rate, momentum,␊
      error function). Some of them are described in~`,
      'E _  2192- 2215 | \\cref{sec:mlp-training}',
      `I _  2215- 2260 |  and the␊
      evaluation results are presented in `,
      'E _  2260- 2299 | \\cref{ch:Optimization-of-System-Design}',
      'I _  2299- 2300 | .',
    ]

## verify samples/papers/write-math-paper/ch1-introduction.tex against snapshot

> Snapshot 1

    [
      `I _     0-   38 | %!TEX root = write-math-ba-paper.tex␊
      ␊
      `,
      'E _    38-   46 | \\section',
      `I _    46-  196 | {Introduction}␊
      On-line recognition makes use of the pen trajectory. One possible␊
      representation of the data is given as groups of sequences of tuples `,
      `E _   196-  224 | $(x, y, t)␊
      \\in \\mathbb{R}^3$`,
      'I _   224-  264 | , where each group represents a stroke, ',
      'E _   264-  272 | $(x, y)$',
      `I _   272-  316 |  is the␊
      position of the pen on a canvas and `,
      'E _   316-  319 | $t$',
      `I _   319-  451 |  is the time.␊
      ␊
      % On-line data was used to classify handwritten natural language text in many␊
      % different variants. For example, the `,
      'E _   451-  469 | $\\text{NPen}^{++}$',
      `I _   469-  581 |  system classified␊
      % cursive handwriting into English words by using hidden Markov models and neural␊
      % networks~`,
      'E _   581-  597 | \\cite{Manke1995}',
      `I _   597-  701 | .␊
      ␊
      % Several systems for mathematical symbol recognition with on-line data have been␊
      % described so far~`,
      'E _   701-  730 | \\cite{Kosmala98,Mouchere2013}',
      `I _   730-  987 | , but no standard test set␊
      % existed to compare the results of different classifiers for single-symbol␊
      % classification of mathematical symbols. The used symbols differed in most␊
      % papers. This is unfortunate as the choice of symbols is crucial for the top-`,
      'E _   987-  990 | $n$',
      `I _   990- 1025 | ␊
      % error. For example, the symbols `,
      'E _  1025- 1028 | $o$',
      'I _  1028- 1030 | , ',
      'E _  1030- 1033 | $O$',
      'I _  1033- 1035 | , ',
      'E _  1035- 1042 | $\\circ$',
      'I _  1042- 1047 |  and ',
      'E _  1047- 1050 | $0$',
      `I _  1050- 1144 |  are very similar and␊
      % systems which know all those classes will certainly have a higher top-`,
      'E _  1144- 1147 | $n$',
      `I _  1147- 1317 |  error␊
      % than systems which only accept one of them. But not only the classes differed,␊
      % also the used data to train and test had to be collected by each author again.␊
      ␊
      `,
      'E _  1317- 1330 | \\cite{Kirsch}',
      `I _  1330- 1469 | ~describes a system called Detexify which uses␊
      time warping to classify on-line handwritten symbols and reports a top-3 error␊
      of less than `,
      'E _  1469- 1488 | $\\SI{10}{\\percent}$',
      'I _  1488- 1502 |  for a set of ',
      'E _  1502- 1513 | $\\num{100}$',
      `I _  1513- 1564 | ~symbols. He did also␊
      recently publish his data on `,
      'E _  1564- 1609 | \\url{https://github.com/kirel/detexify-data},',
      `I _  1609- 1662 | ␊
      which was collected by a crowdsourcing approach via␊
      `,
      'E _  1662- 1697 | \\url{http://detexify.kirelabs.org}.',
      `I _  1697- 1789 |  Those recordings as well as some recordings␊
      which were collected by a similar approach via `,
      'E _  1789- 1816 | \\url{http://write-math.com}',
      `I _  1816- 2048 |  were␊
      merged in a single data set, the labels were semi-automatically checked for␊
      correctness and used to train and evaluated different classifiers. A more␊
      detailed description of all used software, data and experiments is given␊
      in~`,
      'E _  2048- 2065 | \\cite{Thoma:2014}',
      `I _  2065- 2162 | .␊
      ␊
      In this paper we present a baseline system for the classification of on-line␊
      handwriting into `,
      'E _  2162- 2167 | $369$',
      `I _  2167- 2258 |  classes of which some are very similar. An optimized␊
      classifier was developed which has a `,
      'E _  2258- 2279 | $\\SI{29.7}{\\percent}$',
      `I _  2279- 2368 |  relative improvement␊
      of the top-3 error. This was achieved by using better features and `,
      'E _  2368- 2377 | \\gls{SLP}',
      `I _  2377- 2733 | .␊
      The absolute improvements compared to the baseline of those changes will also␊
      be shown.␊
      ␊
      In the following, we will give a general overview of the system design, give␊
      information about the used data and implementation, describe the algorithms␊
      we used to classify the data, report results of our experiments and present␊
      the optimized recognizer we created.`,
    ]

## verify samples/papers/write-math-paper/figures/errors-by-epoch-pretraining/errors-by-epoch-pretraining.tex against snapshot

> Snapshot 1

    [
      'E _     0-   19 | \\begin{tikzpicture}',
      `I _    19-   24 | ␊
          `,
      `E _    24-  712 | \\begin{axis}[␊
                  axis x line=middle,␊
                  axis y line=middle,␊
                  enlarge y limits=true,␊
                  xmin=0,␊
                  % xmax=1000,␊
                  ymin=0.18, ymax=0.4,␊
                  minor ytick={0, 0.01, ..., 1},␊
                  % width=15cm, height=8cm,     % size of the image␊
                  grid = both,␊
                  minor grid style={dashed, gray!30},␊
                  major grid style={gray!40},,␊
                  %grid style={dashed, gray!30},␊
                  ylabel=error,␊
                  xlabel=epoch,␊
                  legend cell align=left,␊
                  legend style={␊
                      at={(0.5,-0.1)},␊
                      anchor=north,␊
                      legend columns=2␊
                  }␊
               ]`,
      `I _   712-  723 | ␊
                `,
      'E _   723-  745 | \\addplot[mark=x,green]',
      `I _   745-  836 |  table [each nth point=20,x=epoch, y=testerror, col sep=comma] {baseline-1.csv};␊
                `,
      'E _   836-  859 | \\addplot[mark=x,orange]',
      `I _   859-  950 |  table [each nth point=20,x=epoch, y=testerror, col sep=comma] {baseline-2.csv};␊
                `,
      'E _   950-  970 | \\addplot[mark=x,red]',
      `I _   970- 1073 |  table [each nth point=20,x=epoch, y=testerror, col sep=comma] {baseline-2-pretraining.csv};␊
                `,
      'E _  1073- 1097 | \\legend{{1 hidden layer}',
      `I _  1097- 1194 | ,␊
                        {2 hidden layers},␊
                        {2 hidden layers with pretraining}}␊
          `,
      'E _  1194- 1204 | \\end{axis}',
      `I _  1204- 1205 | ␊
      `,
      'E _  1205- 1222 | \\end{tikzpicture}',
    ]
